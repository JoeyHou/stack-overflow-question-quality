{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4887d3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.45.0)\n",
      "Collecting sagemaker>=2.31.0\n",
      "  Downloading sagemaker-2.48.0.tar.gz (420 kB)\n",
      "\u001b[K     |████████████████████████████████| 420 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets[s3]==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 57.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.7.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 53.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2.25.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.57.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 71.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.3.3)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.70.11.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (4.0.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.1.5)\n",
      "Collecting botocore==1.19.52\n",
      "  Downloading botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 47.2 MB/s eta 0:00:01     |███████████████████████████████▎| 7.0 MB 47.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3==1.16.43\n",
      "  Downloading boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 51.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (20.3.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (3.15.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (1.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (0.2.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=2.31.0) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.10)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets[s3]==1.6.2) (2021.1)\n",
      "Requirement already satisfied: pox>=0.2.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.31.0) (0.2.9)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.31.0) (1.6.6.3)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from s3fs->datasets[s3]==1.6.2) (1.3.0)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 724 kB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 5.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.7.4.post0)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.12.1)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.7.1)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.6.3)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Building wheels for collected packages: sagemaker, aiobotocore\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.48.0-py2.py3-none-any.whl size=590475 sha256=0e2aade1a3ea4f23665e2b68c14865540a0c200d7b4e612adce6faec15706182\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/16/af/60/39d8f2365d66a0cc11a25193d3818b9754f664064445964b64\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-1.2.2-py3-none-any.whl size=45730 sha256=a1920bdaa910555f11d5441f5de3e2245c5caff7c17ec3d873713c76a9fdc492\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/37/f3/76/dfc2d32494696a7e4710b2f57d9d15212226d19c42dc395865\n",
      "Successfully built sagemaker aiobotocore\n",
      "Installing collected packages: tqdm, botocore, xxhash, s3transfer, huggingface-hub, aiobotocore, tokenizers, sacremoses, datasets, boto3, transformers, sagemaker\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.57.0\n",
      "    Uninstalling tqdm-4.57.0:\n",
      "      Successfully uninstalled tqdm-4.57.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.99\n",
      "    Uninstalling botocore-1.20.99:\n",
      "      Successfully uninstalled botocore-1.20.99\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.4.2\n",
      "    Uninstalling s3transfer-0.4.2:\n",
      "      Successfully uninstalled s3transfer-0.4.2\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 1.3.0\n",
      "    Uninstalling aiobotocore-1.3.0:\n",
      "      Successfully uninstalled aiobotocore-1.3.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.17.99\n",
      "    Uninstalling boto3-1.17.99:\n",
      "      Successfully uninstalled boto3-1.17.99\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.45.0\n",
      "    Uninstalling sagemaker-2.45.0:\n",
      "      Successfully uninstalled sagemaker-2.45.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "spacy 3.0.3 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.8.2 which is incompatible.\n",
      "awscli 1.19.99 requires botocore==1.20.99, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.19.99 requires s3transfer<0.5.0,>=0.4.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "Successfully installed aiobotocore-1.2.2 boto3-1.16.43 botocore-1.19.52 datasets-1.6.2 huggingface-hub-0.0.8 s3transfer-0.3.7 sacremoses-0.0.45 sagemaker-2.48.0 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.6.1 xxhash-2.0.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.31.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e30e5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dd960f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/bert' # The folder we will use for storing data\n",
    "os.makedirs(data_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"bert\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491489a",
   "metadata": {},
   "source": [
    "# 1. Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c50640",
   "metadata": {},
   "source": [
    "## 1.1. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "476b9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open('../data/processed/train_df.pkl', 'rb'))\n",
    "val_df = pickle.load(open('../data/processed/val_df.pkl', 'rb'))\n",
    "test_df = pickle.load(open('../data/processed/test_df.pkl', 'rb'))\n",
    "\n",
    "train_df = pd.concat([train_df, val_df]).reset_index(drop = True)\n",
    "val_df = None\n",
    "\n",
    "train_df = train_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "# val_df = val_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "test_df = test_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "\n",
    "train_df[['label', 'sentence']].to_csv(os.path.join(data_dir, 'train_bert.csv'), index = False, header = True)\n",
    "# val_df[['label', 'sentence']].to_csv(os.path.join(data_dir, 'val_bert.csv'), index = False, header = True)\n",
    "test_df[['label', 'sentence']].to_csv(os.path.join(data_dir, 'test_bert.csv'), index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38b7c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-19fb1072c36d318d\n",
      "Reusing dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-19fb1072c36d318d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-62865b37610d3fc8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/ec2-user/.cache/huggingface/datasets/csv/default-62865b37610d3fc8/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d032b9c829a94b3c8c2083119a23e618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/csv/default-62865b37610d3fc8/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('csv', data_files=os.path.join(data_dir, 'train_bert.csv'))\n",
    "test_dataset = load_dataset('csv', data_files=os.path.join(data_dir, 'test_bert.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66940996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'sentence'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4078c7",
   "metadata": {},
   "source": [
    "## 1.2 Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5137003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edd5523a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fddea0ffe14c49bdc54b006503925f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b03746a56c4cb4980714b6d8bc69f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2e2d0",
   "metadata": {},
   "source": [
    "## 1.3 Uploading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9025bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train_bert'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test_bert'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "# dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385ee32",
   "metadata": {},
   "source": [
    "# 2. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3853bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'per_device_train_batch_size': 2,\n",
    "                 'model_name_or_path': 'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='huggingface_train.py',\n",
    "        source_dir='../src_bert',\n",
    "        instance_type='ml.p2.xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.4',\n",
    "        pytorch_version='1.6',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41336876",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p2.xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-794ad4cbb591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainingJob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConstructed\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcaptures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mall\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstarted\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mretry_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         )\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m     utilize \"us-east-1\" by default if no region can be resolved.\"\"\"\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m     \u001b[0mDEFAULT_ENDPOINT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{service}.{region}.amazonaws.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0m_DUALSTACK_ENABLED_SERVICES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3-control'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_emit_api_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;31m# Given the API params provided by the user and the operation_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;31m# we can serialize the request to a request_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p2.xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedc1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885e230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
