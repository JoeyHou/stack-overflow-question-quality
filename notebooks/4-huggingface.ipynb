{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d954631b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.48.1)\n",
      "Requirement already satisfied: transformers==4.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.6.1)\n",
      "Requirement already satisfied: datasets[s3]==1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.5.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.45)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (20.9)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2.25.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (0.70.11.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (1.1.5)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (0.3.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (2021.4.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (4.0.1)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (2.0.2)\n",
      "Requirement already satisfied: botocore==1.19.43 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (1.19.43)\n",
      "Requirement already satisfied: boto3==1.16.43 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.5.0) (1.16.43)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.5.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.5.0) (0.3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.43->datasets[s3]==1.5.0) (1.26.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.43->datasets[s3]==1.5.0) (2.8.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (3.15.2)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (0.2.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (20.3.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (1.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.31.0) (0.2.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=2.31.0) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.10)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fsspec->datasets[s3]==1.5.0) (0.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets[s3]==1.5.0) (2021.1)\n",
      "Requirement already satisfied: pox>=0.2.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.31.0) (0.2.9)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.31.0) (1.6.6.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.31.0\" \"transformers==4.6.1\" \"datasets[s3]==1.5.0\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "884b0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae336f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/bert' # The folder we will use for storing data\n",
    "os.makedirs(data_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"bert\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0d3ca6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b19d6",
   "metadata": {},
   "source": [
    "# 1. Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7b1bd",
   "metadata": {},
   "source": [
    "## 1.1. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90c3b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c02fa77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pickle.load(open('../data/processed/train_df.pkl', 'rb'))\n",
    "# val_df = pickle.load(open('../data/processed/val_df.pkl', 'rb'))\n",
    "# test_df = pickle.load(open('../data/processed/test_df.pkl', 'rb'))\n",
    "\n",
    "# train_df = pd.concat([train_df, val_df]).reset_index(drop = True)\n",
    "# val_df = None\n",
    "\n",
    "# train_df = train_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "# # val_df = val_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "# test_df = test_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "\n",
    "# train_df[['label', 'sentence']].to_csv(os.path.join(data_dir, 'train_bert.csv'), index = False, header = True)\n",
    "# # val_df[['label', 'sentence']].to_csv(os.path.join(data_dir, 'val_bert.csv'), index = False, header = True)\n",
    "# test_df[['label', 'sentence']].to_csv(os.path.join(data_dir, 'test_bert.csv'), index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba57a9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8b650fbfd11b1ec5\n",
      "Reusing dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-8b650fbfd11b1ec5/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-df64ce23991a7e22\n",
      "Reusing dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-df64ce23991a7e22/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('csv', data_files=os.path.join(data_dir, 'train_bert.csv'))['train']\n",
    "test_dataset = load_dataset('csv', data_files=os.path.join(data_dir, 'test_bert.csv'))['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70c71055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'sentence'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959c40f",
   "metadata": {},
   "source": [
    "## 1.2 Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d0320f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "# dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'sagemaker/stackoverflow-question-quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "063a5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834aeb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'sentence'],\n",
       "    num_rows: 45000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d648eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad5612035db465c83ed846dd0f76e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61a75e2cc274dd3bcd05fb85c1df0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "088cf35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'sentence'],\n",
       "    num_rows: 45000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f6e96",
   "metadata": {},
   "source": [
    "## 1.3 Uploading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199e234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::997893341280:role/service-role/AmazonSageMaker-ExecutionRole-20210707T190726\n",
      "sagemaker bucket: sagemaker-us-east-1-997893341280\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bb060ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train_bert'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test_bert'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841d1b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-997893341280/sagemaker/stackoverflow-question-quality/test_bert'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce9fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# t = load_from_disk(test_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "304a13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "# dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b6e19",
   "metadata": {},
   "source": [
    "# 2. Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d47cc",
   "metadata": {},
   "source": [
    "## 2.1 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38435677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-997893341280/sagemaker/stackoverflow-question-quality/train_bert'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "414ab1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'per_device_train_batch_size': 2,\n",
    "                 'model_name': 'bert-base-uncased'\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='huggingface_train.py',\n",
    "        source_dir='../src_bert',\n",
    "        instance_type='ml.p2.xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.4',\n",
    "        pytorch_version='1.6',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3716c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-10 01:22:32 Starting - Starting the training job...\n",
      "2021-07-10 01:22:54 Starting - Launching requested ML instancesProfilerReport-1625880151: InProgress\n",
      "...\n",
      "2021-07-10 01:23:31 Starting - Preparing the instances for training............\n",
      "2021-07-10 01:25:20 Downloading - Downloading input data......\n",
      "2021-07-10 01:26:16 Training - Downloading the training image......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:06,665 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:06,698 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:06,708 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:07,129 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mCollecting requests==2.22.0\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2021.3.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.0.43)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (4.4.2)\u001b[0m\n",
      "\u001b[34mCollecting idna<2.9,>=2.5\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (0.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers->-r requirements.txt (line 6)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers->-r requirements.txt (line 6)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 6)) (2.4.7)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: idna, requests\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed idna-2.8 requests-2.22.0\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:09,964 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_name\": \"bert-base-uncased\",\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-07-10-01-22-31-590\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-997893341280/huggingface-pytorch-training-2021-07-10-01-22-31-590/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"huggingface_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"huggingface_train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"bert-base-uncased\",\"per_device_train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=huggingface_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=huggingface_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-997893341280/huggingface-pytorch-training-2021-07-10-01-22-31-590/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"bert-base-uncased\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-07-10-01-22-31-590\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-997893341280/huggingface-pytorch-training-2021-07-10-01-22-31-590/source/sourcedir.tar.gz\",\"module_name\":\"huggingface_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"huggingface_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"bert-base-uncased\",\"--per_device_train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=bert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 huggingface_train.py --epochs 1 --model_name bert-base-uncased --per_device_train_batch_size 2\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:14,579 - __main__ - INFO - /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:14,607 - __main__ - INFO -  loaded train_dataset length is: 45000\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:14,607 - __main__ - INFO -  loaded test_dataset length is: 15000\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:14,723 - filelock - INFO - Lock 140458672402272 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:14,746 - filelock - INFO - Lock 140458672402272 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:14,773 - filelock - INFO - Lock 140461180671032 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\u001b[0m\n",
      "\n",
      "2021-07-10 01:30:16 Training - Training image download completed. Training in progress.\u001b[34m2021-07-10 01:30:23,867 - filelock - INFO - Lock 140461180671032 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:36.475 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:36.692 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:36.692 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:36.693 algo-1:32 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:36.833 algo-1:32 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:36.833 algo-1:32 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.061 algo-1:32 INFO hook.py:550] name:bert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.062 algo-1:32 INFO hook.py:550] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.062 algo-1:32 INFO hook.py:550] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.062 algo-1:32 INFO hook.py:550] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.062 algo-1:32 INFO hook.py:550] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.062 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.062 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.062 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.063 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.064 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.065 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.066 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.067 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.068 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.069 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.070 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.071 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.072 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.073 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.074 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.075 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.076 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.077 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.078 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.079 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.080 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.081 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.082 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.083 algo-1:32 INFO hook.py:550] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.084 algo-1:32 INFO hook.py:550] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.084 algo-1:32 INFO hook.py:550] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.084 algo-1:32 INFO hook.py:552] Total Trainable Params: 109483778\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.084 algo-1:32 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:37.086 algo-1:32 INFO hook.py:476] Hook is writing from the hook with pid: 32\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.180 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.180 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.180 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.180 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.180 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.181 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.187 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.188 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.188 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.188 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.188 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.192 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.192 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.193 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.193 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.193 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.194 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.196 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.196 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.196 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.196 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.196 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.201 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.201 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.201 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.201 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.201 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.202 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.204 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.205 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.205 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.205 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.205 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.209 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.209 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.210 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.210 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.210 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.211 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.213 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.213 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.213 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.213 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.213 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.218 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.218 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.218 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.218 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.218 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.219 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.222 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.222 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.222 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.222 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.222 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.227 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.227 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.227 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.227 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.227 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.228 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.230 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.230 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.230 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.230 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.230 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.235 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.235 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.235 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.235 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.236 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.236 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.239 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.239 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.239 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.239 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.239 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.244 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.244 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.244 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.244 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.245 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.245 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.248 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.248 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.248 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.248 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.248 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.253 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.253 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.253 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.253 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.253 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.254 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.256 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.257 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.257 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.257 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.257 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.262 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.262 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.262 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.262 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.262 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.263 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.265 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.265 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.266 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.266 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.266 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.270 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.270 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.271 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.271 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.271 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.272 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.274 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.274 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.274 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.274 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.275 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.279 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.279 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.280 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.280 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.280 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.281 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.283 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.283 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.283 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.283 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.283 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 bool\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.284 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2021-07-10 01:30:38.292 algo-1:32 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:bert BaseModelOutputWithPoolingAndCrossAttentions\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 570/570 [00:00<00:00, 716kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   0%|          | 1.33M/440M [00:00<00:33, 13.1MB/s]#015Downloading:   1%|          | 4.81M/440M [00:00<00:27, 16.1MB/s]#015Downloading:   2%|▏         | 8.06M/440M [00:00<00:22, 19.0MB/s]#015Downloading:   3%|▎         | 13.2M/440M [00:00<00:18, 23.4MB/s]#015Downloading:   4%|▍         | 18.4M/440M [00:00<00:15, 28.0MB/s]#015Downloading:   5%|▌         | 23.7M/440M [00:00<00:12, 32.7MB/s]#015Downloading:   7%|▋         | 29.1M/440M [00:00<00:11, 37.0MB/s]#015Downloading:   8%|▊         | 34.5M/440M [00:00<00:09, 40.9MB/s]#015Downloading:   9%|▉         | 39.9M/440M [00:00<00:09, 44.1MB/s]#015Downloading:  10%|█         | 45.3M/440M [00:01<00:08, 46.7MB/s]#015Downloading:  11%|█▏        | 50.4M/440M [00:01<00:08, 47.5MB/s]#015Downloading:  13%|█▎        | 55.8M/440M [00:01<00:07, 49.4MB/s]#015Downloading:  14%|█▍        | 61.0M/440M [00:01<00:08, 46.2MB/s]#015Downloading:  15%|█▍        | 65.8M/440M [00:01<00:08, 45.4MB/s]#015Downloading:  16%|█▌        | 70.5M/440M [00:01<00:08, 45.9MB/s]#015Downloading:  17%|█▋        | 75.5M/440M [00:01<00:07, 46.8MB/s]#015Downloading:  18%|█▊        | 80.7M/440M [00:01<00:07, 48.4MB/s]#015Downloading:  19%|█▉        | 85.8M/440M [00:01<00:07, 49.1MB/s]#015Downloading:  21%|██        | 91.0M/440M [00:01<00:06, 50.0MB/s]#015Downloading:  22%|██▏       | 96.4M/440M [00:02<00:06, 51.0MB/s]#015Downloading:  23%|██▎       | 102M/440M [00:02<00:06, 50.2MB/s] #015Downloading:  24%|██▍       | 107M/440M [00:02<00:06, 49.7MB/s]#015Downloading:  25%|██▌       | 112M/440M [00:02<00:06, 50.0MB/s]#015Downloading:  27%|██▋       | 117M/440M [00:02<00:06, 51.0MB/s]#015Downloading:  28%|██▊       | 122M/440M [00:02<00:06, 51.0MB/s]#015Downloading:  29%|██▉       | 127M/440M [00:02<00:06, 50.7MB/s]#015Downloading:  30%|███       | 132M/440M [00:02<00:06, 50.5MB/s]#015Downloading:  31%|███       | 137M/440M [00:02<00:06, 50.3MB/s]#015Downloading:  32%|███▏      | 142M/440M [00:02<00:05, 50.3MB/s]#015Downloading:  33%|███▎      | 147M/440M [00:03<00:05, 50.4MB/s]#015Downloading:  35%|███▍      | 153M/440M [00:03<00:05, 50.3MB/s]#015Downloading:  36%|███▌      | 158M/440M [00:03<00:05, 49.9MB/s]#015Downloading:  37%|███▋      | 163M/440M [00:03<00:05, 49.5MB/s]#015Downloading:  38%|███▊      | 168M/440M [00:03<00:05, 49.9MB/s]#015Downloading:  39%|███▉      | 173M/440M [00:03<00:05, 49.7MB/s]#015Downloading:  40%|████      | 178M/440M [00:03<00:05, 49.8MB/s]#015Downloading:  41%|████▏     | 183M/440M [00:03<00:05, 49.3MB/s]#015Downloading:  43%|████▎     | 188M/440M [00:03<00:05, 47.4MB/s]#015Downloading:  44%|████▎     | 193M/440M [00:03<00:05, 48.1MB/s]#015Downloading:  45%|████▍     | 197M/440M [00:04<00:05, 48.5MB/s]#015Downloading:  46%|████▌     | 203M/440M [00:04<00:04, 49.2MB/s]#015Downloading:  47%|████▋     | 207M/440M [00:04<00:04, 48.6MB/s]#015Downloading:  48%|████▊     | 212M/440M [00:04<00:04, 49.0MB/s]#015Downloading:  49%|████▉     | 217M/440M [00:04<00:04, 48.6MB/s]#015Downloading:  50%|█████     | 222M/440M [00:04<00:04, 48.9MB/s]#015Downloading:  52%|█████▏    | 227M/440M [00:04<00:04, 49.4MB/s]#015Downloading:  53%|█████▎    | 232M/440M [00:04<00:04, 49.5MB/s]#015Downloading:  54%|█████▍    | 237M/440M [00:04<00:04, 49.8MB/s]#015Downloading:  55%|█████▌    | 242M/440M [00:04<00:03, 49.6MB/s]#015Downloading:  56%|█████▌    | 247M/440M [00:05<00:04, 44.7MB/s]#015Downloading:  57%|█████▋    | 252M/440M [00:05<00:04, 46.0MB/s]#015Downloading:  58%|█████▊    | 257M/440M [00:05<00:03, 46.6MB/s]#015Downloading:  60%|█████▉    | 262M/440M [00:05<00:03, 48.4MB/s]#015Downloading:  61%|██████    | 267M/440M [00:05<00:03, 48.3MB/s]#015Downloading:  62%|██████▏   | 273M/440M [00:05<00:03, 49.8MB/s]#015Downloading:  63%|██████▎   | 278M/440M [00:05<00:03, 52.1MB/s]#015Downloading:  64%|██████▍   | 284M/440M [00:05<00:02, 52.3MB/s]#015Downloading:  66%|██████▌   | 289M/440M [00:05<00:02, 52.0MB/s]#015Downloading:  67%|██████▋   | 294M/440M [00:06<00:02, 52.0MB/s]#015Downloading:  68%|██████▊   | 300M/440M [00:06<00:02, 52.5MB/s]#015Downloading:  69%|██████▉   | 305M/440M [00:06<00:02, 52.5MB/s]#015Downloading:  70%|███████   | 310M/440M [00:06<00:02, 51.0MB/s]#015Downloading:  72%|███████▏  | 315M/440M [00:06<00:02, 48.7MB/s]#015Downloading:  73%|███████▎  | 320M/440M [00:06<00:02, 43.2MB/s]#015Downloading:  74%|███████▍  | 325M/440M [00:06<00:02, 45.7MB/s]#015Downloading:  75%|███████▌  | 331M/440M [00:06<00:02, 47.4MB/s]#015Downloading:  76%|███████▋  | 336M/440M [00:06<00:02, 48.7MB/s]#015Downloading:  77%|███████▋  | 341M/440M [00:07<00:02, 49.5MB/s]#015Downloading:  79%|███████▊  | 346M/440M [00:07<00:01, 50.0MB/s]#015Downloading:  80%|███████▉  | 351M/440M [00:07<00:01, 50.5MB/s]#015Downloading:  81%|████████  | 356M/440M [00:07<00:01, 50.7MB/s]#015Downloading:  82%|████████▏ | 362M/440M [00:07<00:01, 49.9MB/s]#015Downloading:  83%|████████▎ | 367M/440M [00:07<00:01, 50.6MB/s]#015Downloading:  84%|████████▍ | 372M/440M [00:07<00:01, 50.6MB/s]#015Downloading:  86%|████████▌ | 377M/440M [00:07<00:01, 50.9MB/s]#015Downloading:  87%|████████▋ | 383M/440M [00:07<00:01, 52.3MB/s]#015Downloading:  88%|████████▊ | 388M/440M [00:07<00:00, 52.7MB/s]#015Downloading:  89%|████████▉ | 393M/440M [00:08<00:00, 52.6MB/s]#015Downloading:  90%|█████████ | 399M/440M [00:08<00:00, 52.7MB/s]#015Downloading:  92%|█████████▏| 404M/440M [00:08<00:00, 52.1MB/s]#015Downloading:  93%|█████████▎| 409M/440M [00:08<00:00, 51.9MB/s]#015Downloading:  94%|█████████▍| 414M/440M [00:08<00:00, 50.8MB/s]#015Downloading:  95%|█████████▌| 420M/440M [00:08<00:00, 51.4MB/s]#015Downloading:  96%|█████████▋| 425M/440M [00:08<00:00, 51.5MB/s]#015Downloading:  98%|█████████▊| 430M/440M [00:08<00:00, 51.3MB/s]#015Downloading:  99%|█████████▉| 435M/440M [00:08<00:00, 51.4MB/s]#015Downloading: 100%|█████████▉| 440M/440M [00:08<00:00, 49.8MB/s]#015Downloading: 100%|██████████| 440M/440M [00:08<00:00, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5625 [00:00<?, ?it/s]/codebuild/output/src811146734/src/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"huggingface_train.py\", line 83, in <module>\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1053, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1443, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1475, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 750, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 1526, in forward\n",
      "    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 754, in _call_impl\n",
      "    hook_result = hook(self, input, result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 450, in forward_hook\n",
      "    self._write_outputs(module_name, outputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 1050, in _write_outputs\n",
      "    name, outputs, CallbackHook.OUTPUT_TENSOR_SUFFIX, idx=idx\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 1026, in _write\n",
      "    self._save_for_tensor(module_name + suffix + str(idx), var)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 881, in _save_for_tensor\n",
      "    self._write_for_tensor(tensor_name, tensor_value, save_collections_for_tensor)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 926, in _write_for_tensor\n",
      "    self._write_raw_tensor(tensor_name, tensor_value, save_collections, tensor_ref=tensor_ref)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 808, in _write_raw_tensor\n",
      "    self._write_raw_tensor_simple(tensor_name, tensor_value, tensor_ref=tensor_ref)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 839, in _write_raw_tensor_simple\n",
      "    numpy_tensor_value = self._make_numpy_array(tensor_value)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 639, in _make_numpy_array\n",
      "    return tensor_value.to(torch.device(\"cpu\")).data.numpy()\u001b[0m\n",
      "\u001b[34mRuntimeError: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5625 [00:02<?, ?it/s]\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-10 01:30:40,295 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.6 huggingface_train.py --epochs 1 --model_name bert-base-uncased --per_device_train_batch_size 2\"\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 570/570 [00:00<00:00, 716kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   0%|          | 1.33M/440M [00:00<00:33, 13.1MB/s]#015Downloading:   1%|          | 4.81M/440M [00:00<00:27, 16.1MB/s]#015Downloading:   2%|â         | 8.06M/440M [00:00<00:22, 19.0MB/s]#015Downloading:   3%|â         | 13.2M/440M [00:00<00:18, 23.4MB/s]#015Downloading:   4%|â         | 18.4M/440M [00:00<00:15, 28.0MB/s]#015Downloading:   5%|â         | 23.7M/440M [00:00<00:12, 32.7MB/s]#015Downloading:   7%|â         | 29.1M/440M [00:00<00:11, 37.0MB/s]#015Downloading:   8%|â         | 34.5M/440M [00:00<00:09, 40.9MB/s]#015Downloading:   9%|â         | 39.9M/440M [00:00<00:09, 44.1MB/s]#015Downloading:  10%|â         | 45.3M/440M [00:01<00:08, 46.7MB/s]#015Downloading:  11%|ââ        | 50.4M/440M [00:01<00:08, 47.5MB/s]#015Downloading:  13%|ââ        | 55.8M/440M [00:01<00:07, 49.4MB/s]#015Downloading:  14%|ââ        | 61.0M/440M [00:01<00:08, 46.2MB/s]#015Downloading:  15%|ââ        | 65.8M/440M [00:01<00:08, 45.4MB/s]#015Downloading:  16%|ââ        | 70.5M/440M [00:01<00:08, 45.9MB/s]#015Downloading:  17%|ââ        | 75.5M/440M [00:01<00:07, 46.8MB/s]#015Downloading:  18%|ââ        | 80.7M/440M [00:01<00:07, 48.4MB/s]#015Downloading:  19%|ââ        | 85.8M/440M [00:01<00:07, 49.1MB/s]#015Downloading:  21%|ââ        | 91.0M/440M [00:01<00:06, 50.0MB/s]#015Downloading:  22%|âââ       | 96.4M/440M [00:02<00:06, 51.0MB/s]#015Downloading:  23%|âââ       | 102M/440M [00:02<00:06, 50.2MB/s] #015Downloading:  24%|âââ       | 107M/440M [00:02<00:06, 49.7MB/s]#015Downloading:  25%|âââ       | 112M/440M [00:02<00:06, 50.0MB/s]#015Downloading:  27%|âââ       | 117M/440M [00:02<00:06, 51.0MB/s]#015Downloading:  28%|âââ       | 122M/440M [00:02<00:06, 51.0MB/s]#015Downloading:  29%|âââ       | 127M/440M [00:02<00:06, 50.7MB/s]#015Downloading:  30%|âââ       | 132M/440M [00:02<00:06, 50.5MB/s]#015Downloading:  31%|âââ       | 137M/440M [00:02<00:06, 50.3MB/s]#015Downloading:  32%|ââââ      | 142M/440M [00:02<00:05, 50.3MB/s]#015Downloading:  33%|ââââ      | 147M/440M [00:03<00:05, 50.4MB/s]#015Downloading:  35%|ââââ      | 153M/440M [00:03<00:05, 50.3MB/s]#015Downloading:  36%|ââââ      | 158M/440M [00:03<00:05, 49.9MB/s]#015Downloading:  37%|ââââ      | 163M/440M [00:03<00:05, 49.5MB/s]#015Downloading:  38%|ââââ      | 168M/440M [00:03<00:05, 49.9MB/s]#015Downloading:  39%|ââââ      | 173M/440M [00:03<00:05, 49.7MB/s]#015Downloading:  40%|ââââ      | 178M/440M [00:03<00:05, 49.8MB/s]#015Downloading:  41%|âââââ     | 183M/440M [00:03<00:05, 49.3MB/s]#015Downloading:  43%|âââââ     | 188M/440M [00:03<00:05, 47.4MB/s]#015Downloading:  44%|âââââ     | 193M/440M [00:03<00:05, 48.1MB/s]#015Downloading:  45%|âââââ     | 197M/440M [00:04<00:05, 48.5MB/s]#015Downloading:  46%|âââââ     | 203M/440M [00:04<00:04, 49.2MB/s]#015Downloading:  47%|âââââ     | 207M/440M [00:04<00:04, 48.6MB/s]#015Downloading:  48%|âââââ     | 212M/440M [00:04<00:04, 49.0MB/s]#015Downloading:  49%|âââââ     | 217M/440M [00:04<00:04, 48.6MB/s]#015Downloading:  50%|âââââ     | 222M/440M [00:04<00:04, 48.9MB/s]#015Downloading:  52%|ââââââ    | 227M/440M [00:04<00:04, 49.4MB/s]#015Downloading:  53%|ââââââ    | 232M/440M [00:04<00:04, 49.5MB/s]#015Downloading:  54%|ââââââ    | 237M/440M [00:04<00:04, 49.8MB/s]#015Downloading:  55%|ââââââ    | 242M/440M [00:04<00:03, 49.6MB/s]#015Downloading:  56%|ââââââ    | 247M/440M [00:05<00:04, 44.7MB/s]#015Downloading:  57%|ââââââ    | 252M/440M [00:05<00:04, 46.0MB/s]#015Downloading:  58%|ââââââ    | 257M/440M [00:05<00:03, 46.6MB/s]#015Downloading:  60%|ââââââ    | 262M/440M [00:05<00:03, 48.4MB/s]#015Downloading:  61%|ââââââ    | 267M/440M [00:05<00:03, 48.3MB/s]#015Downloading:  62%|âââââââ   | 273M/440M [00:05<00:03, 49.8MB/s]#015Downloading:  63%|âââââââ   | 278M/440M [00:05<00:03, 52.1MB/s]#015Downloading:  64%|âââââââ   | 284M/440M [00:05<00:02, 52.3MB/s]#015Downloading:  66%|âââââââ   | 289M/440M [00:05<00:02, 52.0MB/s]#015Downloading:  67%|âââââââ   | 294M/440M [00:06<00:02, 52.0MB/s]#015Downloading:  68%|âââââââ   | 300M/440M [00:06<00:02, 52.5MB/s]#015Downloading:  69%|âââââââ   | 305M/440M [00:06<00:02, 52.5MB/s]#015Downloading:  70%|âââââââ   | 310M/440M [00:06<00:02, 51.0MB/s]#015Downloading:  72%|ââââââââ  | 315M/440M [00:06<00:02, 48.7MB/s]#015Downloading:  73%|ââââââââ  | 320M/440M [00:06<00:02, 43.2MB/s]#015Downloading:  74%|ââââââââ  | 325M/440M [00:06<00:02, 45.7MB/s]#015Downloading:  75%|ââââââââ  | 331M/440M [00:06<00:02, 47.4MB/s]#015Downloading:  76%|ââââââââ  | 336M/440M [00:06<00:02, 48.7MB/s]#015Downloading:  77%|ââââââââ  | 341M/440M [00:07<00:02, 49.5MB/s]#015Downloading:  79%|ââââââââ  | 346M/440M [00:07<00:01, 50.0MB/s]#015Downloading:  80%|ââââââââ  | 351M/440M [00:07<00:01, 50.5MB/s]#015Downloading:  81%|ââââââââ  | 356M/440M [00:07<00:01, 50.7MB/s]#015Downloading:  82%|âââââââââ | 362M/440M [00:07<00:01, 49.9MB/s]#015Downloading:  83%|âââââââââ | 367M/440M [00:07<00:01, 50.6MB/s]#015Downloading:  84%|âââââââââ | 372M/440M [00:07<00:01, 50.6MB/s]#015Downloading:  86%|âââââââââ | 377M/440M [00:07<00:01, 50.9MB/s]#015Downloading:  87%|âââââââââ | 383M/440M [00:07<00:01, 52.3MB/s]#015Downloading:  88%|âââââââââ | 388M/440M [00:07<00:00, 52.7MB/s]#015Downloading:  89%|âââââââââ | 393M/440M [00:08<00:00, 52.6MB/s]#015Downloading:  90%|âââââââââ | 399M/440M [00:08<00:00, 52.7MB/s]#015Downloading:  92%|ââââââââââ| 404M/440M [00:08<00:00, 52.1MB/s]#015Downloading:  93%|ââââââââââ| 409M/440M [00:08<00:00, 51.9MB/s]#015Downloading:  94%|ââââââââââ| 414M/440M [00:08<00:00, 50.8MB/s]#015Downloading:  95%|ââââââââââ| 420M/440M [00:08<00:00, 51.4MB/s]#015Downloading:  96%|ââââââââââ| 425M/440M [00:08<00:00, 51.5MB/s]#015Downloading:  98%|ââââââââââ| 430M/440M [00:08<00:00, 51.3MB/s]#015Downloading:  99%|ââââââââââ| 435M/440M [00:08<00:00, 51.4MB/s]#015Downloading: 100%|ââââââââââ| 440M/440M [00:08<00:00, 49.8MB/s]#015Downloading: 100%|ââââââââââ| 440M/440M [00:08<00:00, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5625 [00:00<?, ?it/s]/codebuild/output/src811146734/src/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"huggingface_train.py\", line 83, in <module>\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1053, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1443, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1475, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 750, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 1526, in forward\n",
      "    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 754, in _call_impl\n",
      "    hook_result = hook(self, input, result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 450, in forward_hook\n",
      "    self._write_outputs(module_name, outputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 1050, in _write_outputs\n",
      "    name, outputs, CallbackHook.OUTPUT_TENSOR_SUFFIX, idx=idx\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 1026, in _write\n",
      "    self._save_for_tensor(module_name + suffix + str(idx), var)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 881, in _save_for_tensor\n",
      "    self._write_for_tensor(tensor_name, tensor_value, save_collections_for_tensor)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 926, in _write_for_tensor\n",
      "    self._write_raw_tensor(tensor_name, tensor_value, save_collections, tensor_ref=tensor_ref)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 808, in _write_raw_tensor\n",
      "    self._write_raw_tensor_simple(tensor_name, tensor_value, tensor_ref=tensor_ref)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/core/hook.py\", line 839, in _write_raw_tensor_simple\n",
      "    numpy_tensor_value = self._make_numpy_array(tensor_value)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 639, in _make_numpy_array\n",
      "    return tensor_value.to(torch.device(\"cpu\")).data.numpy()\u001b[0m\n",
      "\u001b[34mRuntimeError: CUDA error: device-side assert triggered\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5625 [00:02<?, ?it/s]\u001b[0m\n",
      "\n",
      "2021-07-10 01:30:59 Uploading - Uploading generated training model\n",
      "2021-07-10 01:30:59 Failed - Training job failed\n",
      "ProfilerReport-1625880151: Stopping\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2021-07-10-01-22-31-590: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 huggingface_train.py --epochs 1 --model_name bert-base-uncased --per_device_train_batch_size 2\"\n\rDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 570/570 [00:00<00:00, 716kB/s]\n\rDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\rDownloading:   0%|          | 1.33M/440M [00:00<00:33, 13.1MB/s]\rDownloading:   1%|          | 4.81M/440M [00:00<00:27, 16.1MB/s]\rDownloading:   2%|â         | 8.06M/440M [00:00<00:22, 19.0MB/s]\rDownloading:   3%|â         | 13.2M/440M [00:00<00:18, 23.4MB/s]\rDownloading:   4%|â         | 18.4M/440M [00:00<00:15, 28.0MB/s]\rDownloading:   5%|â         | 23.7M/440M [00:00<00:12, 32.7MB/s]\rDownloading:   7%|â         | 29.1M/440M [00:00<00:11, 37.0MB/s]\rDownloading:   8%|â         | 34.5M/440M [00:00<00:09, 40.9MB/s]\rDownloading:   9%|â         | 39.9M/440M [00:00<00:09, 44.1MB/s]\rDownloading:  10%|â         | 45.3M/440M [00:01<00:08, 46.7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-794ad4cbb591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1625\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3733\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3734\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3291\u001b[0m                 ),\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3293\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3294\u001b[0m             )\n\u001b[1;32m   3295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2021-07-10-01-22-31-590: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 huggingface_train.py --epochs 1 --model_name bert-base-uncased --per_device_train_batch_size 2\"\n\rDownloading:   0%|          | 0.00/570 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 570/570 [00:00<00:00, 716kB/s]\n\rDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\rDownloading:   0%|          | 1.33M/440M [00:00<00:33, 13.1MB/s]\rDownloading:   1%|          | 4.81M/440M [00:00<00:27, 16.1MB/s]\rDownloading:   2%|â         | 8.06M/440M [00:00<00:22, 19.0MB/s]\rDownloading:   3%|â         | 13.2M/440M [00:00<00:18, 23.4MB/s]\rDownloading:   4%|â         | 18.4M/440M [00:00<00:15, 28.0MB/s]\rDownloading:   5%|â         | 23.7M/440M [00:00<00:12, 32.7MB/s]\rDownloading:   7%|â         | 29.1M/440M [00:00<00:11, 37.0MB/s]\rDownloading:   8%|â         | 34.5M/440M [00:00<00:09, 40.9MB/s]\rDownloading:   9%|â         | 39.9M/440M [00:00<00:09, 44.1MB/s]\rDownloading:  10%|â         | 45.3M/440M [00:01<00:08, 46.7"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb921fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed245750",
   "metadata": {},
   "source": [
    "## 2.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4435ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_transformer = huggingface_estimator.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate result:\n",
    "# open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
