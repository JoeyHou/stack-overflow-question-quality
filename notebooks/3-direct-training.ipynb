{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5962bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe940415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.3.1) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.57.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# need torch 1.3.1 for elastic inference\n",
    "!pip install torch==1.3.1\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd588c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6d97d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "######### Important Variables #########\n",
    "# max length of the sentence\n",
    "MAX_LEN = 128 \n",
    "\n",
    "# BERT Tokenizer\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3f557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0a35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f3b5b4",
   "metadata": {},
   "source": [
    "# 1. Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bf07908",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open('../data/train_df.pkl', 'rb'))\n",
    "val_df = pickle.load(open('../data/val_df.pkl', 'rb'))\n",
    "test_df = pickle.load(open('../data/test_df.pkl', 'rb'))\n",
    "\n",
    "train_df = train_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "val_df = val_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "test_df = test_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470c6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a6d139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d412586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataloader(dataset):\n",
    "    if dataset == 'train':\n",
    "        data = pickle.load(open('../data/train_df.pkl', 'rb'))\n",
    "    elif dataset == 'val':\n",
    "        data = pickle.load(open('../data/val_df.pkl', 'rb'))#.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "    else:\n",
    "        data = pickle.load(open('../data/test_df.pkl', 'rb'))\n",
    "    data = data.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "    \n",
    "    sentences = data.sentence.values\n",
    "    labels = data.label.values\n",
    "    \n",
    "    # 2. Encode text\n",
    "    input_ids = [tokenizer.encode(sent, add_special_tokens = True) for sent in tqdm(sentences)]\n",
    "    \n",
    "    # 3. Pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "    \n",
    "    # 4. Adding mask; mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    \n",
    "    # 5. Convert to PyTorch data types.\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    labels = torch.tensor(labels)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    if dataset == 'test':\n",
    "        sampler = SequentialSampler(data)\n",
    "    else:\n",
    "        sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7e131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8594b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 39/35000 [00:00<01:30, 387.26it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2365 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 35000/35000 [01:50<00:00, 317.37it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 144 at dim 1 (got 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-17354204e9fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b3160b701e54>\u001b[0m in \u001b[0;36mprep_dataloader\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# 5. Convert to PyTorch data types.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 144 at dim 1 (got 128)"
     ]
    }
   ],
   "source": [
    "train_dataloader = prep_dataloader('train')\n",
    "val_dataloader = prep_dataloader('val')\n",
    "test_dataloader = prep_dataloader('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680194a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d013d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e9acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f775a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83024585",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac148a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=args.num_labels,  # The number of output labels--2 for binary classification.\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 4Training settings (optimizer, distributed computing, etc.)\n",
    "# if is_distributed and use_cuda:\n",
    "#     # multi-machine multi-gpu case\n",
    "#     model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "# else:\n",
    "#     # single-machine multi-gpu case or single-machine or multi-machine cpu case\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "    eps=1e-8,  # args.adam_epsilon - default is 1e-8.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65741827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    eval_accuracy = 0\n",
    "    \n",
    "    all_pred = []\n",
    "    all_label = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            \n",
    "            for pred in np.argmax(logits, axis=1).flatten():\n",
    "                all_pred.append(pred)\n",
    "            for label in label_ids.flatten():\n",
    "                all_label.append(label)\n",
    "            #tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            #eval_accuracy += tmp_eval_accuracy\n",
    "    all_pred = np.array(all_pred)\n",
    "    all_label = np.array(all_label)\n",
    "    eval_accuracy = np.sum(all_pred == all_label) / len(all_label)\n",
    "    return eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epoches = []\n",
    "num_epoches = 4\n",
    "for epoch in range(1, num_epoches + 1):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in tqdm(enumerate(train_loader)):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_acc = test(model, train_loader, device)\n",
    "    val_acc = test(model, val_loader, device)\n",
    "    all_epoches.append({\n",
    "        'epoch': epoch,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'model': model\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf4fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d274375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1da139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
