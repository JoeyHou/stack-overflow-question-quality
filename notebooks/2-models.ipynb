{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aa0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603f930",
   "metadata": {},
   "source": [
    "# 1. Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b8176f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open('../data/train_df.pkl', 'rb'))\n",
    "val_df = pickle.load(open('../data/val_df.pkl', 'rb'))\n",
    "test_df = pickle.load(open('../data/test_df.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da455d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['title_length', 'body_length']]\n",
    "X_val = val_df[['title_length', 'body_length']]\n",
    "X_test = test_df[['title_length', 'body_length']]\n",
    "\n",
    "y_train = train_df.cleaned_y.values\n",
    "y_val = val_df.cleaned_y.values\n",
    "y_test = test_df.cleaned_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df46eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "for c in tqdm([0.001, 0.01, 0.1, 1, 10, 100, 1000]):\n",
    "    clf = LogisticRegression(random_state = 0).fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    val_acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "    all_rows.append({\n",
    "        'c': c,\n",
    "        'train_acc': round(train_acc, 2),\n",
    "        'val_acc': round(val_acc, 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a969854a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.000</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          c  train_acc  val_acc\n",
       "0     0.001       0.39     0.39\n",
       "1     0.010       0.39     0.39\n",
       "2     0.100       0.39     0.39\n",
       "3     1.000       0.39     0.39\n",
       "4    10.000       0.39     0.39\n",
       "5   100.000       0.39     0.39\n",
       "6  1000.000       0.39     0.39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf29d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3d9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ed582ee",
   "metadata": {},
   "source": [
    "# 2. Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d967cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.3.1\n",
      "  Downloading torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 734.6 MB 7.4 kB/s  eta 0:00:01     |███████                         | 161.0 MB 85.8 MB/s eta 0:00:07     |█████████████████▉              | 408.7 MB 82.0 MB/s eta 0:00:04��█▎            | 441.7 MB 51.1 MB/s eta 0:00:06     |███████████████████████████▌    | 630.6 MB 69.8 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.3.1) (1.19.5)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Successfully installed torch-1.3.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 73.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.57.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 61.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# need torch 1.3.1 for elastic inference\n",
    "!pip install torch==1.3.1\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb6a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c1163",
   "metadata": {},
   "source": [
    "## 2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bf3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/stackoverflow-question-quality\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212c81e",
   "metadata": {},
   "source": [
    "## 2.2 Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57435368",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open('../data/train_df.pkl', 'rb'))\n",
    "val_df = pickle.load(open('../data/val_df.pkl', 'rb'))\n",
    "test_df = pickle.load(open('../data/test_df.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0e3295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Y</th>\n",
       "      <th>cleaned_y</th>\n",
       "      <th>body_notag</th>\n",
       "      <th>all_text</th>\n",
       "      <th>title_length</th>\n",
       "      <th>body_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26373</th>\n",
       "      <td>47236918</td>\n",
       "      <td>click edit button any record will fetch first ...</td>\n",
       "      <td>I have a problem\\r\\nWhen I click the Edit butt...</td>\n",
       "      <td>&lt;php&gt;&lt;html&gt;&lt;mysql&gt;&lt;sql&gt;</td>\n",
       "      <td>2017-11-11 10:31:22</td>\n",
       "      <td>LQ_EDIT</td>\n",
       "      <td>1</td>\n",
       "      <td>have problem click edit button any record will...</td>\n",
       "      <td>click edit button any record will fetch first ...</td>\n",
       "      <td>12</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>43043808</td>\n",
       "      <td>to filter username harmful characters</td>\n",
       "      <td>&lt;p&gt;i really need help from you guys !!\\nsome o...</td>\n",
       "      <td>&lt;php&gt;&lt;html&gt;</td>\n",
       "      <td>2017-03-27 10:21:57</td>\n",
       "      <td>LQ_CLOSE</td>\n",
       "      <td>0</td>\n",
       "      <td>really need help you guys of friends scamming ...</td>\n",
       "      <td>to filter username harmful characters really n...</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id                                              Title  \\\n",
       "26373  47236918  click edit button any record will fetch first ...   \n",
       "18718  43043808              to filter username harmful characters   \n",
       "\n",
       "                                                    Body  \\\n",
       "26373  I have a problem\\r\\nWhen I click the Edit butt...   \n",
       "18718  <p>i really need help from you guys !!\\nsome o...   \n",
       "\n",
       "                          Tags         CreationDate         Y  cleaned_y  \\\n",
       "26373  <php><html><mysql><sql>  2017-11-11 10:31:22   LQ_EDIT          1   \n",
       "18718              <php><html>  2017-03-27 10:21:57  LQ_CLOSE          0   \n",
       "\n",
       "                                              body_notag  \\\n",
       "26373  have problem click edit button any record will...   \n",
       "18718  really need help you guys of friends scamming ...   \n",
       "\n",
       "                                                all_text  title_length  \\\n",
       "26373  click edit button any record will fetch first ...            12   \n",
       "18718  to filter username harmful characters really n...             5   \n",
       "\n",
       "       body_length  \n",
       "26373           90  \n",
       "18718           26  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb89644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "val_df = val_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "test_df = test_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aad7de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['label', 'sentence']].to_csv('../data/train_s3.csv', index = False, header = True)\n",
    "val_df[['label', 'sentence']].to_csv('../data/val_s3.csv', index = False, header = True)\n",
    "test_df[['label', 'sentence']].to_csv('../data/test_s3.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb21595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"../data/train_s3.csv\", bucket=bucket, key_prefix=prefix)\n",
    "inputs_val = sagemaker_session.upload_data(\"../data/val_s3.csv\", bucket=bucket, key_prefix=prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"../data/test_s3.csv\", bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf5d2f",
   "metadata": {},
   "source": [
    "## 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7039a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# # place to save model artifact\n",
    "# output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "# estimator = PyTorch(\n",
    "#     entry_point=\"train.py\",\n",
    "#     source_dir=\"../src/\",\n",
    "#     role=role,\n",
    "#     framework_version=\"1.3.1\",\n",
    "#     py_version=\"py3\",\n",
    "#     instance_count=2,  # this script only support distributed training for GPU instances.\n",
    "#     instance_type=\"ml.p3.2xlarge\",\n",
    "#     output_path=output_path,\n",
    "#     hyperparameters={\n",
    "#         \"epochs\": 2,\n",
    "#         \"num_labels\": 2,\n",
    "#         \"backend\": \"gloo\",\n",
    "#     },\n",
    "#     disable_profiler=True, # disable debugger\n",
    "# )\n",
    "# estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'per_device_train_batch_size': 32,\n",
    "                 'model_name_or_path': 'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.4',\n",
    "        #pytorch_version='1.6',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8fc5d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.fit(\n",
    "  {'train': bucket + prefix,\n",
    "   'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ef80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket + prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd97db8",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b8df7",
   "metadata": {},
   "source": [
    "## 3.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7eaf136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open('../data/train_df.pkl', 'rb'))\n",
    "val_df = pickle.load(open('../data/val_df.pkl', 'rb'))\n",
    "test_df = pickle.load(open('../data/test_df.pkl', 'rb'))\n",
    "\n",
    "train_df = train_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "val_df = val_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "test_df = test_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6481017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, val_df]).reset_index(drop = True)\n",
    "val_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d2f7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dfc75",
   "metadata": {},
   "source": [
    "## 3.2 Convert sentences to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b9290ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def question_to_words(question):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    #print(review)\n",
    "    #text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", question.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b483a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_to_words(train_df.sentence.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "38805e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click edit button any record will fetch first data please inform thank have problem click edit button any record will fetch first data please inform thank have problem click edit button any record will fetch first data please inform thank have problem click edit button any record will fetch first data please inform thank query query mysqli_query conn select customer order cu_id desc die mysqli_error echo echo cu id cu name email phone coun id card id update delete row mysqli_fetch_assoc query echo row cu_id row cu_name row email row phone row coun_id row card_id edit delete echo echo echo mysqli_close conn'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sentence.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68dfb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(\"../cache\", \"stackoverflow-question-quality\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5784df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = [question_to_words(question) for question in tqdm(data_train)]\n",
    "        words_test = [question_to_words(question) for question in tqdm(data_test)]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46e0cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = train_df.sentence.values, train_df.label.values\n",
    "test_X, test_y = test_df.sentence.values, test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0415791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 15713/45000 [04:21<07:53, 61.91it/s]"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f352fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = test_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c719d",
   "metadata": {},
   "source": [
    "## 3.3 List of words to word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a17cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for sent in tqdm(data):\n",
    "        for word in sent:\n",
    "            if word in word_count.keys():\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    \n",
    "    sorted_words = [pair[0] for pair in sorted(word_count.items(), key = lambda x: x[1], reverse = True)]\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283552a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45000/45000 [00:01<00:00, 31406.77it/s]\n"
     ]
    }
   ],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5078c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/lstm' # The folder we will use for storing data\n",
    "os.makedirs(data_dir, exist_ok=True)  # ensure cache directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced5a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643f76f",
   "metadata": {},
   "source": [
    "## 3.4 Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43b793cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=2000):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=2000):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in tqdm(data):\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39fd15d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45000/45000 [00:02<00:00, 21472.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "# test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d456bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X = test_X_len = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22c2dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = test_df = None\n",
    "word_dict = None\n",
    "test_X = test_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b390b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62494f7b",
   "metadata": {},
   "source": [
    "## 3.5 Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34742d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df = pd.DataFrame(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa6d7e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>549</td>\n",
       "      <td>66</td>\n",
       "      <td>456</td>\n",
       "      <td>498</td>\n",
       "      <td>89</td>\n",
       "      <td>19</td>\n",
       "      <td>70</td>\n",
       "      <td>395</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420</td>\n",
       "      <td>287</td>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>431</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "      <td>1025</td>\n",
       "      <td>1375</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>245</td>\n",
       "      <td>135</td>\n",
       "      <td>42</td>\n",
       "      <td>19</td>\n",
       "      <td>1225</td>\n",
       "      <td>83</td>\n",
       "      <td>39</td>\n",
       "      <td>423</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4892</td>\n",
       "      <td>11</td>\n",
       "      <td>767</td>\n",
       "      <td>968</td>\n",
       "      <td>965</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>443</td>\n",
       "      <td>441</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>195</td>\n",
       "      <td>705</td>\n",
       "      <td>204</td>\n",
       "      <td>863</td>\n",
       "      <td>14</td>\n",
       "      <td>186</td>\n",
       "      <td>106</td>\n",
       "      <td>18</td>\n",
       "      <td>376</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  1990  \\\n",
       "0   132   549    66   456   498    89    19    70   395    62  ...     0   \n",
       "1   420   287     1   295   431    39    41  1025  1375     1  ...     0   \n",
       "2     1   245   135    42    19  1225    83    39   423     1  ...     0   \n",
       "3  4892    11   767   968   965    20     1   443   441    11  ...     0   \n",
       "4    79   195   705   204   863    14   186   106    18   376  ...     0   \n",
       "\n",
       "   1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85b08b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)\n",
    "# pd.concat([pd.DataFrame(test_y), pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f5f9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/stackoverflow-question-quality'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ada3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d5ac9",
   "metadata": {},
   "source": [
    "## 3.6 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f28c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"../src_lstm\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    py_version='py3', # Joey: not sure if this is expected here\n",
    "                    hyperparameters={\n",
    "                        'epochs': 20,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db0a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-07 02:58:00 Starting - Starting the training job...\n",
      "2021-07-07 02:58:24 Starting - Launching requested ML instancesProfilerReport-1625626679: InProgress\n",
      "......\n",
      "2021-07-07 02:59:24 Starting - Preparing the instances for training............\n",
      "2021-07-07 03:01:25 Downloading - Downloading input data...\n",
      "2021-07-07 03:01:45 Training - Downloading the training image...\n",
      "2021-07-07 03:02:25 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:22,463 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:22,489 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:28,719 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:28,968 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:28,968 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:28,968 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:28,968 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/c0/d1/ad6afa6000ab869f6af2c85985d40558ffb298d9fcb2ab04c0648436008f/regex-2021.7.6.tar.gz (693kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/7a/ec/f8ff3ccfc4e59ce619a66a0bf29dc3b49c2e8c07de29d572e191c006eaa2/tqdm-4.61.2-py2.py3-none-any.whl (76kB)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train, regex\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vezrx2ay/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/4d/21/94/ffc1c84ddb509f51dab71898a63df9fbff5f1e04552ee8ea8e\u001b[0m\n",
      "\u001b[34mSuccessfully built train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, regex, tqdm, joblib, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\u001b[0m\n",
      "\u001b[34m  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.9.3 html5lib-1.1 joblib-0.14.1 nltk-3.6.2 numpy-1.18.5 pandas-0.24.2 pytz-2021.1 regex-2021.7.6 soupsieve-2.1 tqdm-4.61.2 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-07-07 03:02:51,159 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 200,\n",
      "        \"epochs\": 20\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"num_gpus\": 1,\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-07-07-02-57-59-806\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-039506837819/sagemaker-pytorch-2021-07-07-02-57-59-806/source/sourcedir.tar.gz\",\n",
      "    \"log_level\": 20\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":20,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-039506837819/sagemaker-pytorch-2021-07-07-02-57-59-806/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":20,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-07-07-02-57-59-806\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-039506837819/sagemaker-pytorch-2021-07-07-02-57-59-806/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"20\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 20 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
      "\u001b[34mEpoch: 1, Cross Entropy: 1.0694698745554143\u001b[0m\n",
      "\u001b[34mEpoch: 2, Cross Entropy: 1.0145220438187772\u001b[0m\n",
      "\u001b[34mEpoch: 3, Cross Entropy: 0.9781976748596538\u001b[0m\n",
      "\u001b[34mEpoch: 4, Cross Entropy: 0.9702434492382136\u001b[0m\n",
      "\u001b[34mEpoch: 5, Cross Entropy: 0.9197756424546242\u001b[0m\n",
      "\u001b[34mEpoch: 6, Cross Entropy: 0.9161648553880778\u001b[0m\n",
      "\u001b[34mEpoch: 7, Cross Entropy: 0.9144751131534576\u001b[0m\n",
      "\u001b[34mEpoch: 8, Cross Entropy: 0.9078534082932905\u001b[0m\n",
      "\u001b[34mEpoch: 9, Cross Entropy: 0.9078005403280258\u001b[0m\n",
      "\u001b[34mEpoch: 10, Cross Entropy: 0.9139108143069528\u001b[0m\n",
      "\u001b[34mEpoch: 11, Cross Entropy: 0.9206210327419367\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025e8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7744482",
   "metadata": {},
   "source": [
    "## 3.7 Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bd7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d14ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdd00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affec72c",
   "metadata": {},
   "source": [
    "## 3.8 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ba4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 11159/45000 [00:01<00:04, 8020.04it/s]"
     ]
    }
   ],
   "source": [
    "# word_dict = pickle.load(open('../data/lstm/word_dict.pkl', \"rb\"))\n",
    "\n",
    "train_df = pickle.load(open('../data/train_df.pkl', 'rb'))\n",
    "val_df = pickle.load(open('../data/val_df.pkl', 'rb'))\n",
    "test_df = pickle.load(open('../data/test_df.pkl', 'rb'))\n",
    "\n",
    "train_df = train_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "val_df = val_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "test_df = test_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "\n",
    "train_df = pd.concat([train_df, val_df]).reset_index(drop = True)\n",
    "val_df = None\n",
    "\n",
    "\n",
    "test_X, test_y = test_df.sentence.values, test_df.label.values\n",
    "train_X, train_y = train_df.sentence.values, train_df.label.values\n",
    "\n",
    "word_dict = build_dict(train_X)\n",
    "\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)\n",
    "\n",
    "\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f6165ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_y = None\n",
    "test_df = train_df = None\n",
    "word_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "774ee79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)\n",
    "# np.array(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cc6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cc87c79",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15000, 45000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-43c245f597b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 263\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15000, 45000]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0691d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
