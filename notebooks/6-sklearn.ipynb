{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "center-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-question",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preceding-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/sklearn' # The folder we will use for storing data\n",
    "os.makedirs(data_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"lstm\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-relative",
   "metadata": {},
   "source": [
    "# 1. Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-bacon",
   "metadata": {},
   "source": [
    "## 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "brown-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open('../data/processed/train_df.pkl', 'rb'))\n",
    "val_df = pickle.load(open('../data/processed/val_df.pkl', 'rb'))\n",
    "test_df = pickle.load(open('../data/processed/test_df.pkl', 'rb'))\n",
    "\n",
    "train_df = train_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "val_df = val_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "test_df = test_df.rename(columns = {'cleaned_y': 'label', 'all_text': 'sentence'})\n",
    "\n",
    "train_df = pd.concat([train_df, val_df]).reset_index(drop = True)\n",
    "val_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "solar-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-thumbnail",
   "metadata": {},
   "source": [
    "## 1.2 Question to List of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "large-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def question_to_words(question):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    #print(review)\n",
    "    #text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", question.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "native-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = [question_to_words(question) for question in tqdm(data_train)]\n",
    "        words_test = [question_to_words(question) for question in tqdm(data_test)]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "vanilla-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = train_df.sentence.values, train_df.label.values\n",
    "test_X, test_y = test_df.sentence.values, test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "funky-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "touched-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = test_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "editorial-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-praise",
   "metadata": {},
   "source": [
    "## 1.3 Extract BOW features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "absent-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.externals \n",
    "import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "split-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(cache_dir, \"bow_features.pkl\"), \"rb\") as f:\n",
    "#     cache_data = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "immune-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "spoken-individual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-corps",
   "metadata": {},
   "source": [
    "- **Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "lightweight-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test_sklearn.csv'), header=False, index=False)\n",
    "test_X = vocabulary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-carrier",
   "metadata": {},
   "source": [
    "- **Validation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ongoing-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5000), (10000, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx = np.random.choice(range(len(train_y)), size=10000, replace=False)\n",
    "\n",
    "val_X = pd.DataFrame(train_X[val_idx])\n",
    "val_y = pd.DataFrame(train_y[val_idx])\n",
    "\n",
    "val_X.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "athletic-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'val_sklearn.csv'), header=False, index=False)\n",
    "val_y = val_X = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-ordering",
   "metadata": {},
   "source": [
    "- **Train Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "classified-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = [i for i in range(len(train_y)) if i not in val_idx]\n",
    "\n",
    "train_X = pd.DataFrame(train_X[train_idx])\n",
    "train_y = pd.DataFrame(train_y[train_idx])\n",
    "\n",
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train_sklearn.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "excessive-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-firewall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "suspected-collector",
   "metadata": {},
   "source": [
    "## 1.4 Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "level-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "prefix = 'sagemaker/stackoverflow-question-quality'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test_sklearn.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'val_sklearn.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train_sklearn.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caroline-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_location = session.upload_data(os.path.join(data_dir, 'test_sklearn.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "light-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = pd.read_csv(data_dir + '/test_sklearn.csv', header = None)\n",
    "# t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "authorized-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t[5].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "temporal-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-announcement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "apparent-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = pd.read_csv(data_dir + '/val_sklearn.csv', header = None)\n",
    "# t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-ethnic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tight-confidentiality",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accepted-hotel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.48.1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "proved-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "affiliated-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "container = image_uris.retrieve(framework='xgboost',region='us-east-1',version='1.2-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "genuine-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/sklearn_output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=11,\n",
    "                        eta=0.2622829413333343,\n",
    "                        gamma=1.0375026465214707,\n",
    "                        min_child_weight=3,\n",
    "                        subsample=0.8544514211613925,\n",
    "                        #silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_class=3,\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vulnerable-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = TrainingInput(\"s3://{}/{}/{}\".format(bucket, prefix, 'train_sklearn.csv'), content_type='csv')\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}\".format(bucket, prefix, 'val_sklearn.csv'), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "spatial-eclipse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-997893341280/sagemaker/stackoverflow-question-quality/train_sklearn.csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"s3://{}/{}/{}\".format(bucket, prefix, 'train_sklearn.csv')# validation_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cordless-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-09 04:12:06 Starting - Starting the training job...\n",
      "2021-07-09 04:12:29 Starting - Launching requested ML instancesProfilerReport-1625803925: InProgress\n",
      "...\n",
      "2021-07-09 04:13:04 Starting - Preparing the instances for training............\n",
      "2021-07-09 04:14:50 Downloading - Downloading input data...\n",
      "2021-07-09 04:15:30 Training - Downloading the training image...\n",
      "2021-07-09 04:15:57 Training - Training image download completed. Training in progress..\u001b[34m[2021-07-09 04:16:00.002 ip-10-2-133-222.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 35000 rows and 5000 columns\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 10000 rows\u001b[0m\n",
      "\u001b[34m[04:16:10] WARNING: ../src/learner.cc:516: \u001b[0m\n",
      "\u001b[34mParameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[0]#011train-merror:0.36260#011validation-merror:0.39520\u001b[0m\n",
      "\u001b[34m[1]#011train-merror:0.34520#011validation-merror:0.38280\u001b[0m\n",
      "\u001b[34m[2]#011train-merror:0.32477#011validation-merror:0.36810\u001b[0m\n",
      "\u001b[34m[3]#011train-merror:0.31649#011validation-merror:0.36180\u001b[0m\n",
      "\u001b[34m[4]#011train-merror:0.30580#011validation-merror:0.35860\u001b[0m\n",
      "\u001b[34m[5]#011train-merror:0.30037#011validation-merror:0.35090\u001b[0m\n",
      "\u001b[34m[6]#011train-merror:0.29326#011validation-merror:0.34440\u001b[0m\n",
      "\u001b[34m[7]#011train-merror:0.28509#011validation-merror:0.33980\u001b[0m\n",
      "\u001b[34m[8]#011train-merror:0.27826#011validation-merror:0.33380\u001b[0m\n",
      "\u001b[34m[9]#011train-merror:0.27354#011validation-merror:0.33120\u001b[0m\n",
      "\u001b[34m[10]#011train-merror:0.26646#011validation-merror:0.32770\u001b[0m\n",
      "\u001b[34m[11]#011train-merror:0.26271#011validation-merror:0.32740\u001b[0m\n",
      "\u001b[34m[12]#011train-merror:0.25800#011validation-merror:0.32190\u001b[0m\n",
      "\u001b[34m[13]#011train-merror:0.25343#011validation-merror:0.32220\u001b[0m\n",
      "\u001b[34m[14]#011train-merror:0.25069#011validation-merror:0.32030\u001b[0m\n",
      "\u001b[34m[15]#011train-merror:0.24560#011validation-merror:0.31700\u001b[0m\n",
      "\u001b[34m[16]#011train-merror:0.24226#011validation-merror:0.31510\u001b[0m\n",
      "\u001b[34m[17]#011train-merror:0.23877#011validation-merror:0.31220\u001b[0m\n",
      "\u001b[34m[18]#011train-merror:0.23466#011validation-merror:0.30960\u001b[0m\n",
      "\u001b[34m[19]#011train-merror:0.23214#011validation-merror:0.30900\u001b[0m\n",
      "\u001b[34m[20]#011train-merror:0.22920#011validation-merror:0.30940\u001b[0m\n",
      "\u001b[34m[21]#011train-merror:0.22634#011validation-merror:0.30760\u001b[0m\n",
      "\u001b[34m[22]#011train-merror:0.22263#011validation-merror:0.30660\u001b[0m\n",
      "\u001b[34m[23]#011train-merror:0.22006#011validation-merror:0.30630\u001b[0m\n",
      "\u001b[34m[24]#011train-merror:0.21634#011validation-merror:0.30490\u001b[0m\n",
      "\u001b[34m[25]#011train-merror:0.21423#011validation-merror:0.30320\u001b[0m\n",
      "\u001b[34m[26]#011train-merror:0.21220#011validation-merror:0.30240\u001b[0m\n",
      "\u001b[34m[27]#011train-merror:0.20960#011validation-merror:0.30160\u001b[0m\n",
      "\u001b[34m[28]#011train-merror:0.20683#011validation-merror:0.30160\u001b[0m\n",
      "\u001b[34m[29]#011train-merror:0.20437#011validation-merror:0.30080\u001b[0m\n",
      "\u001b[34m[30]#011train-merror:0.20186#011validation-merror:0.30060\u001b[0m\n",
      "\u001b[34m[31]#011train-merror:0.20071#011validation-merror:0.30050\u001b[0m\n",
      "\u001b[34m[32]#011train-merror:0.19911#011validation-merror:0.30080\u001b[0m\n",
      "\u001b[34m[33]#011train-merror:0.19694#011validation-merror:0.29890\u001b[0m\n",
      "\u001b[34m[34]#011train-merror:0.19603#011validation-merror:0.29720\u001b[0m\n",
      "\u001b[34m[35]#011train-merror:0.19483#011validation-merror:0.29900\u001b[0m\n",
      "\u001b[34m[36]#011train-merror:0.19317#011validation-merror:0.29770\u001b[0m\n",
      "\u001b[34m[37]#011train-merror:0.19071#011validation-merror:0.29740\u001b[0m\n",
      "\u001b[34m[38]#011train-merror:0.19006#011validation-merror:0.29590\u001b[0m\n",
      "\u001b[34m[39]#011train-merror:0.18840#011validation-merror:0.29450\u001b[0m\n",
      "\u001b[34m[40]#011train-merror:0.18763#011validation-merror:0.29420\u001b[0m\n",
      "\u001b[34m[41]#011train-merror:0.18586#011validation-merror:0.29340\u001b[0m\n",
      "\u001b[34m[42]#011train-merror:0.18537#011validation-merror:0.29300\u001b[0m\n",
      "\u001b[34m[43]#011train-merror:0.18440#011validation-merror:0.29240\u001b[0m\n",
      "\u001b[34m[44]#011train-merror:0.18314#011validation-merror:0.29150\u001b[0m\n",
      "\u001b[34m[45]#011train-merror:0.18197#011validation-merror:0.29150\u001b[0m\n",
      "\u001b[34m[46]#011train-merror:0.18054#011validation-merror:0.29170\u001b[0m\n",
      "\u001b[34m[47]#011train-merror:0.17911#011validation-merror:0.29140\u001b[0m\n",
      "\u001b[34m[48]#011train-merror:0.17717#011validation-merror:0.28960\u001b[0m\n",
      "\u001b[34m[49]#011train-merror:0.17577#011validation-merror:0.29050\u001b[0m\n",
      "\u001b[34m[50]#011train-merror:0.17486#011validation-merror:0.29080\u001b[0m\n",
      "\u001b[34m[51]#011train-merror:0.17303#011validation-merror:0.28920\u001b[0m\n",
      "\u001b[34m[52]#011train-merror:0.17214#011validation-merror:0.28780\u001b[0m\n",
      "\u001b[34m[53]#011train-merror:0.17086#011validation-merror:0.28580\u001b[0m\n",
      "\u001b[34m[54]#011train-merror:0.16971#011validation-merror:0.28490\u001b[0m\n",
      "\u001b[34m[55]#011train-merror:0.16831#011validation-merror:0.28430\u001b[0m\n",
      "\u001b[34m[56]#011train-merror:0.16769#011validation-merror:0.28380\u001b[0m\n",
      "\u001b[34m[57]#011train-merror:0.16680#011validation-merror:0.28360\u001b[0m\n",
      "\u001b[34m[58]#011train-merror:0.16577#011validation-merror:0.28370\u001b[0m\n",
      "\u001b[34m[59]#011train-merror:0.16454#011validation-merror:0.28320\u001b[0m\n",
      "\u001b[34m[60]#011train-merror:0.16380#011validation-merror:0.28300\u001b[0m\n",
      "\u001b[34m[61]#011train-merror:0.16237#011validation-merror:0.28280\u001b[0m\n",
      "\u001b[34m[62]#011train-merror:0.16094#011validation-merror:0.28270\u001b[0m\n",
      "\u001b[34m[63]#011train-merror:0.16043#011validation-merror:0.28080\u001b[0m\n",
      "\u001b[34m[64]#011train-merror:0.15983#011validation-merror:0.28060\u001b[0m\n",
      "\u001b[34m[65]#011train-merror:0.15866#011validation-merror:0.28050\u001b[0m\n",
      "\u001b[34m[66]#011train-merror:0.15851#011validation-merror:0.28080\u001b[0m\n",
      "\u001b[34m[67]#011train-merror:0.15774#011validation-merror:0.28120\u001b[0m\n",
      "\u001b[34m[68]#011train-merror:0.15629#011validation-merror:0.28110\u001b[0m\n",
      "\u001b[34m[69]#011train-merror:0.15523#011validation-merror:0.28130\u001b[0m\n",
      "\u001b[34m[70]#011train-merror:0.15466#011validation-merror:0.28090\u001b[0m\n",
      "\u001b[34m[71]#011train-merror:0.15454#011validation-merror:0.28090\u001b[0m\n",
      "\u001b[34m[72]#011train-merror:0.15411#011validation-merror:0.28130\u001b[0m\n",
      "\u001b[34m[73]#011train-merror:0.15291#011validation-merror:0.28120\u001b[0m\n",
      "\u001b[34m[74]#011train-merror:0.15166#011validation-merror:0.28060\u001b[0m\n",
      "\u001b[34m[75]#011train-merror:0.15074#011validation-merror:0.28130\u001b[0m\n",
      "\n",
      "2021-07-09 04:36:56 Uploading - Uploading generated training model\n",
      "2021-07-09 04:36:56 Completed - Training job completed\n",
      "ProfilerReport-1625803925: NoIssuesFound\n",
      "Training seconds: 1309\n",
      "Billable seconds: 1309\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-breakdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "elementary-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"s3://{}/{}/{}\".format(bucket, prefix, 'val_sklearn.csv')\n",
    "test_location = 's3://sagemaker-us-east-1-997893341280/sagemaker/stackoverflow-question-quality/test_sklearn.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "norman-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "boring-airplane",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................\u001b[34m[2021-07-09:07:54:16:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:16:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:16:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-07-09 07:54:17 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2021-07-09 07:54:17 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[34m[2021-07-09 07:54:17 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-07-09 07:54:17 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2021-07-09 07:54:17 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2021-07-09 07:54:17 +0000] [31] [INFO] Booting worker with pid: 31\u001b[0m\n",
      "\u001b[34m[2021-07-09 07:54:17 +0000] [32] [INFO] Booting worker with pid: 32\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:22 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:22 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:25:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:25:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2021-07-09T07:54:22.837:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:27 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:27 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:28 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:28 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:28 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:28 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:28 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:28 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:31 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:34 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:34 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:37 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:40 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:40 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:40 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:40 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:41 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-07-09:07:54:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:40 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:40 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:41 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-07-09:07:54:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2132 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2132 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [09/Jul/2021:07:54:43 +0000] \"POST /invocations HTTP/1.1\" 200 2516 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "architectural-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "colored-scope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-997893341280/sagemaker-xgboost-2021-07-09-07-47-55-844'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb_transformer.wait()\n",
    "xgb_transformer.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "plain-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sklearn'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "arranged-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(xgb_transformer.output_path, 'test_sklearn.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "basic-chaos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7146666666666667"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "actual-rally",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "super-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !$xgb_transformer.output_path\n",
    "# bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "white-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp\n",
    "# data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-cotton",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dirty-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "average-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/sklearn_output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        #silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_class=3,\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "upper-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:merror', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 6, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'max_depth': IntegerParameter(3, 12),\n",
    "                                                    'eta'      : ContinuousParameter(0.05, 0.5),\n",
    "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
    "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
    "                                                    'gamma': ContinuousParameter(0, 10),\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "behavioral-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = TrainingInput(\"s3://{}/{}/{}\".format(bucket, prefix, 'train_sklearn.csv'), content_type='csv')\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}\".format(bucket, prefix, 'val_sklearn.csv'), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "shared-prison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "furnished-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_location = 's3://sagemaker-us-east-1-997893341280/sagemaker/stackoverflow-question-quality/test_sklearn.csv'\n",
    "# best_training_job_location = 's3://sagemaker-us-east-1-997893341280/sagemaker/stackoverflow-question-quality/sklearn_output/sagemaker-xgboost-210708-0851-006-1c702236/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "graduate-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-07-08 15:51:00 Starting - Preparing the instances for training\n",
      "2021-07-08 15:51:00 Downloading - Downloading input data\n",
      "2021-07-08 15:51:00 Training - Training image download completed. Training in progress.\n",
      "2021-07-08 15:51:00 Uploading - Uploading generated training model\n",
      "2021-07-08 15:51:00 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "xgb_attached = sagemaker.estimator.Estimator.attach('sagemaker-xgboost-2021-07-08-14-02-27-286')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interesting-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer = xgb_attached.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "saving-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_input = TrainingInput(\"s3://{}/{}/{}\".format(bucket, prefix, 'test_sklearn.csv'), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-command",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............."
     ]
    }
   ],
   "source": [
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shared-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(xgb_hyperparameter_tuner, open('../data/sklearn/xgb_hyperparameter_tuner.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test_sklearn.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-scale",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-peeing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
